import torch
from transformers import PreTrainedTokenizer
from .jamo_utils import get_last_syllable_jamo
from .hash_policy import HashPolicy

class JamoWatermarkDetector:
    """
    Detects and extracts a watermark from text generated by JamoWatermarkProcessor.
    """
    def __init__(self, tokenizer: PreTrainedTokenizer, original_message: str, mode: str = 'robustness', k_bits: int = 2):
        self.tokenizer = tokenizer
        self.mode = mode
        self.k_bits = k_bits
        self.step_t = 0
        # The detector must know the payload to check if the extracted bits match the target.
        byte_data = original_message.encode('utf-8')
        self.payload = ''.join(format(byte, '08b') for byte in byte_data)
        self.hash_policy = HashPolicy(mode=self.mode, k_bits=self.k_bits)

    def _extract_bits_from_token(self, token_str: str) -> int | None:
        """
        Extracts the embedded bits from a single token based on the current step.
        """
        # 1. Decompose the last syllable of the token into Jamo indices
        jamo_indices = get_last_syllable_jamo(token_str)
        
        # If the token has no Hangul syllable, it couldn't have been watermarked.
        if jamo_indices is None:
            return None

        # 2. Select the channel (Choseong, Jungseong, Jongseong) based on the current step
        # This must follow the same round-robin logic as the processor.
        channel_idx = self.step_t % 3

        # 3. Calculate the hashes for all three channels
        token_hashes = self.hash_policy.calculate_channel_hashes(*jamo_indices)

        # 4. The extracted bits are the hash value of the selected channel
        extracted_bits = token_hashes[channel_idx]
        
        return extracted_bits

    def extract_payload(self, input_ids: torch.LongTensor) -> str:
        """
        Extracts the full watermark payload from a sequence of token IDs.
        """
        self.step_t = 0 # Reset step counter for each new text
        extracted_payload = ""
        
        # Decode the entire sequence once, then split into tokens (re-tokenization; note_251107)
        token_ids = input_ids[0].tolist()

        for token_id in token_ids:
            # Skip special tokens like BOS/EOS
            if token_id in self.tokenizer.all_special_ids:
                continue

            # Use convert_ids_to_tokens to get the raw token string,
            # which prevents re-tokenization issues (e.g., ' 인' -> '인').
            # This ensures consistency with the generation process.
            token_str = self.tokenizer.convert_ids_to_tokens(token_id)
            extracted_bits = self._extract_bits_from_token(token_str)

            # This is the core synchronization logic for the detector.
            if extracted_bits is not None: # If the token contains a Hangul syllable
                # Get the target bits that should have been embedded at this step
                if self.step_t * self.k_bits >= len(self.payload):
                    break # Stop if we are beyond the payload length
                target_bits = int(self.payload[self.step_t * self.k_bits : (self.step_t + 1) * self.k_bits], 2)

                # Check if the extracted bits match the target bits
                if extracted_bits == target_bits:
                    # This token was successfully watermarked. Append the bits and advance the step.
                    binary_representation = format(extracted_bits, f'0{self.k_bits}b')
                    extracted_payload += binary_representation
                    self.step_t += 1
                # If it does not match, do nothing. The step_t remains the same,
                # and we will check the next token against the same target_bits.

        return extracted_payload